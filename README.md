# dendgau
dend udacity
# Project 1: Data Modeling with Postgres

---

### Project Background

---

Sparkify, a music streaming startup, wants to analyze the data they've collected on songs and user behavior on their new app. The analytics team is curious about the music that customers listen to. They need a pipeline to query their data, which is stored in the directories of JSON logs on user activity and on the songs in the app.

### Project Goal

---

Development of an ETL pipeline in `Python` to load data from `JSON` files to a database in `PosgreSQL`.

### Project Objectives {#project-obj}

---

1. Create a star-schema data warehouse in `PosgreSQL`.
2. Open the `JSON` dataset files in `Python` from a directory.
3. Load `JSON` dataset to a `pandas` *DataFrame*.
4. Select required columns from a *DataFrame*.
5. Populate a `PostgreSQL` table with selected columns.
6. Iterate for each table.

### Datasets

---

- `Songs Dataset` - a subset of real data from the [Million Song Dataset].
Sample Record :
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```
- `Log Dataset` is generated by the event simulator based on the songs in the dataset.
Sample Record :
```
{"artist": null, "auth": "Logged In", "firstName": "Walter", "gender": "M", "itemInSession": 0, "lastName": "Frye", "length": null, "level": "free", "location": "San Francisco-Oakland-Hayward, CA", "method": "GET","page": "Home", "registration": 1540919166796.0, "sessionId": 38, "song": null, "status": 200, "ts": 1541105830796, "userAgent": "\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"", "userId": "39"}
```

### Data Warehouse

---
##### Star Schema
![Schema Pic](/img/star_schema.png "Schema")
In this project a star schema is applied to develop data warehouse. A star schema is the best way to speed up a data analysis process within a data warehouse due to its denormalized tables. Denormalized tables reduce the number of joins, which is one of the bottlenecks when applying data analysis in OLTP systems. A star schema is a flexible design that can be changed easily throughout the development cycle as users demands grow.

##### Fact Table

`songplays` - records in log data associated with song plays i.e. records with page NextSong
|songplay_id|start_time |user_id    |level      |artist_id  |session_id |location   |user_agent |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
|SERIAL     |TIMESTAMP  |INT        |VARCHAR    |VARCHAR    |VARCHAR    |TEXT       |TEXT       |
|PRIMARY KEY|           |NOT NULL   |           |           |           |           |           |

##### Dimension Tables

`users` - users records in the app
|user_id        |first_name     |last_name      |gender         |level      |
|---------------|---------------|---------------|---------------|-----------|
|SERIAL         |VARCHAR        |VARCHAR        |Custom type    |VARCHAR    |
|PRIMARY KEY    |               |               |DEFAULT NULL   |           |

`songs` - songs in music database
|song_id    |title      |artist_id  |year       |duration   |
|-----------|-----------|-----------|-----------|-----------|
|VARCHAR    |VARCHAR    |VARCHAR    |INT        |FLOAT      |
|PRIMARY KEY|NOT NULL   |           |           |           |
 
`artists` - artists in music database
|artist_id  |name       |location   |latitude   |longitude  |
|-----------|-----------|-----------|-----------|-----------|
|VARCHAR    |VARCHAR    |VARCHAR    |FLOAT      |FLOAT      |
|PRIMARY KEY|           |           |           |           |

`time` - timestamps of records in songplays broken down into specific units
|start_time |hour       |day        |week       |month      |year       |weekday    |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
|TIMESTAMP  |INT        |INT        |INT        |INT        |INT        |INT        |
|PRIMARY KEY|           |           |           |           |           |           |

### Project Files

---

* `create_tables.py` drops and creates tables in `PostgreSQL`. Run this file to reset the tables before each time you run your ETL scripts.
* `sql_queries.py` contains all sql queries (CREATE, DROP, INSERT, SELECT)
* `etl.py` reads and processes files from ***song_data*** and ***log_data*** and loads them into the tables.
* `test.ipynb` displays the first few rows of each table to check the database.
* `etl.ipynb `reads and processes a single file from ***song_data*** and ***log_data*** and loads the data into the tables.

### Run Scripts

---

Steps to run an ETL process in order to achieve goals listed in [project objectives](#project-obj) section:

1. Open the Terminal.
2. Navigate to the project's folder (with `cd /you_directory_path`).
3. Run `create_tables.py` to create or drop the tables by typing the following in the Terminal:

   ```python
   python create_tables.py
   ```

4. Run `etl.py` to activate ETL process:

   ```python
   python etl.py
   ```

5. Run `test.ipynb` notebook to validate tables.

[//]: # (These are reference links used in the body of this note and get stripped out when the markdown processor does its job. There is no need to format nicely because it shouldn't be seen. Thanks SO - http://stackoverflow.com/questions/4823468/store-comments-in-markdown-syntax)

   [Million Song Dataset]: <http://millionsongdataset.com/>

